{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet') \n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization Imports\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline \n",
    "%config InlineBackend.figure_formats = ['retina']\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.set_palette('husl')\n",
    "sns.set_context('talk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load pickled dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./tokenized_wine_data_w_target.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of tokenized descriptions\n",
    "corpus = list(df['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stop words\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words from corpus\n",
    "clean_corpus = []\n",
    "\n",
    "for desc in corpus:\n",
    "    desc = [word for word in desc if word not in stop_words]\n",
    "    clean_corpus.append(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize words\n",
    "clean_corpus = [[WordNetLemmatizer().lemmatize(word) for word in desc] for desc in clean_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corpus[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset-specific stop words\n",
    "wine_stopwords = ['alongside', 'aroma', 'palate', 'offer', 'hint', 'include', \n",
    "                  'offering', 'recall', 'pretty', 'nose', 'note', 'lightly', \n",
    "                  'part', 'extended', 'series', 'show', 'backed', 'touch', \n",
    "                  'flavor', 'provides', 'companion', 'behind', 'mouthfeel', \n",
    "                  'could', 'plus', 'open', 'background', 'tone', 'stand', \n",
    "                  'isnt', 'expressive', 'mouth', 'wine', 'broad', 'generous', \n",
    "                  'term', 'would', 'make', 'tiny', 'blend']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words from corpus\n",
    "cleaner_corpus = []\n",
    "\n",
    "for desc in clean_corpus:\n",
    "    desc = [word for word in desc if word not in wine_stopwords]\n",
    "    cleaner_corpus.append(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner_corpus[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rejoin lists of words in each description for use in CV & TF-IDF\n",
    "cleaner_corpus_joined = []\n",
    "\n",
    "for doc in cleaner_corpus:\n",
    "    joined = ' '.join(doc)\n",
    "    cleaner_corpus_joined.append(joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Goal: come up with most important vocabulary list for wine descriptions (aka distill wine descriptions down to most important parts) --> Figures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Then, build model that can determine varietal based on description (use variety as target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ModelSomm: Finally, build model that can determine varietal + Province based off description as a proxy for taste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Document-Term Matrix from Wine Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()#ngram_range=(1,2))\n",
    "X_cv = cv.fit_transform(cleaner_corpus_joined)\n",
    "\n",
    "print(f\"Dimensions of Document-term matrix: {X_cv.toarray().shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checked out the vocab list\n",
    "# cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfvec = TfidfVectorizer()#stop_words = 'english')#ngram_range=(1,2))\n",
    "X_tfidf = tfidfvec.fit_transform(clean_corpus_joined)\n",
    "\n",
    "print(f\"Dimensions of Document-term matrix: {X_tfidf.toarray().shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each document in doc list into a list of lowercase tokens\n",
    "#tokenized_docs = [gensim.utils.simple_preprocess(d) for d in clean_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gensim Dictionary.  This creates an id to word mapping for everything in our vocbulary\n",
    "# It is NOT the same as the dictionary object in the Python standard library\n",
    "# index is key, value is word\n",
    "mydict = gensim.corpora.Dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gensim Corpus object.  This creates a list of tuples for each document.\n",
    "# The first element of the tuple is the word id, the second is the number of counts\n",
    "mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in clean_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tf-idf model\n",
    "tfidf = gensim.models.TfidfModel(mycorpus)\n",
    "tfidf_matrix = gensim.matutils.corpus2dense(tfidf[mycorpus], num_terms=len(mydict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size: desired dimension of our word vectors\n",
    "# window: size of our context window\n",
    "# sg: using Skip-gram architecture\n",
    "\n",
    "model = gensim.models.Word2Vec(clean_corpus, size=10, window=2, min_count=1, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('bright', topn=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PCA for Scree Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing number of components with a scree plot\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=200)\n",
    "pca.fit(X_tfidf)\n",
    "pcafeatures_train = pca.transform(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.xlabel('# components')\n",
    "plt.ylabel('explained variance');\n",
    "plt.title('Scree plot for digits dataset');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('# components')\n",
    "plt.ylabel('cumulative explained variance');\n",
    "plt.title('Cumulative explained variance by PCA for digits');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
